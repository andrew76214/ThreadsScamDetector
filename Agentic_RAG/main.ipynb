{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/ollama_platform/TaiwanScamDetect/Agentic_RAG/config.py:10: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  LOCAL_LLM = ChatOllama(model=\"llama3.2:3b\", format=\"json\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "from utils import process_and_store_documents, build_graph\n",
    "from config import LOCAL_LLM, URLS\n",
    "from config import RetrievalGrader_prompt, generate_prompt, HallucinationGrader_prompt, AnswerGrader_prompt, Router_prompt\n",
    "from nodes import Nodes\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ollama_platform/TaiwanScamDetect/Agentic_RAG/utils.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "/home/ollama_platform/TaiwanScamDetect/Agentic_RAG/utils.py:36: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "/home/ollama_platform/miniconda3/envs/AgenticRAG/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "retriever = process_and_store_documents(URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n            of a retrieved document to a user question. If the document contains keywords related to the user question, \\n            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n            \\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n            \\n            Here is the retrieved document: \\n            Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n            \\n            Here is the user question: \\n            agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [62ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:09:13.106484295Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 59899017,\n",
      "          \"load_duration\": 8892791,\n",
      "          \"prompt_eval_count\": 329,\n",
      "          \"prompt_eval_duration\": 19000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 31000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:09:13.106484295Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 59899017,\n",
      "              \"load_duration\": 8892791,\n",
      "              \"prompt_eval_count\": 329,\n",
      "              \"prompt_eval_duration\": 19000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 31000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3d301839-b80d-49f4-bb9b-5d8d8a82be56-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [63ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "retrieval_grader = RetrievalGrader_prompt | LOCAL_LLM | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: agent memory \\n    Context: [Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454732006214860835, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733409373126691, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733618843222051, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733703172325411, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [162ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:09:13.284795811Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 159632835,\n",
      "          \"load_duration\": 10431070,\n",
      "          \"prompt_eval_count\": 2048,\n",
      "          \"prompt_eval_duration\": 96000000,\n",
      "          \"eval_count\": 2,\n",
      "          \"eval_duration\": 51000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:09:13.284795811Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 159632835,\n",
      "              \"load_duration\": 10431070,\n",
      "              \"prompt_eval_count\": 2048,\n",
      "              \"prompt_eval_duration\": 96000000,\n",
      "              \"eval_count\": 2,\n",
      "              \"eval_duration\": 51000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-149b98c7-840b-40ef-914f-97627dd4629e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [164ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}\"\n",
      "}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = generate_prompt | LOCAL_LLM | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454732006214860835, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733409373126691, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733618843222051, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'pk': 454733703172325411, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\"}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [162ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:09:13.452891765Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 160711471,\n",
      "          \"load_duration\": 10416099,\n",
      "          \"prompt_eval_count\": 2048,\n",
      "          \"prompt_eval_duration\": 98000000,\n",
      "          \"eval_count\": 2,\n",
      "          \"eval_duration\": 51000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:09:13.452891765Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 160711471,\n",
      "              \"load_duration\": 10416099,\n",
      "              \"prompt_eval_count\": 2048,\n",
      "              \"prompt_eval_duration\": 98000000,\n",
      "              \"eval_count\": 2,\n",
      "              \"eval_duration\": 51000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-03957006-cb96-4cf1-ba16-5c6649a408e1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [164ms] Exiting Chain run with output:\n",
      "\u001b[0m{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_grader = HallucinationGrader_prompt | LOCAL_LLM | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"{}\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"{}\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    {} \\n\\n    Here is the question: agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [50ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:09:13.509708846Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47871941,\n",
      "          \"load_duration\": 8756333,\n",
      "          \"prompt_eval_count\": 106,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 36000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:09:13.509708846Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47871941,\n",
      "              \"load_duration\": 8756333,\n",
      "              \"prompt_eval_count\": 106,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 36000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-391caa74-8c76-4469-95f0-102aeaf603a1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [51ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_grader = AnswerGrader_prompt | LOCAL_LLM | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    llm agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [63ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:09:13.619268895Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 61937053,\n",
      "          \"load_duration\": 9228609,\n",
      "          \"prompt_eval_count\": 146,\n",
      "          \"prompt_eval_duration\": 4000000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 47000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:09:13.619268895Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 61937053,\n",
      "              \"load_duration\": 9228609,\n",
      "              \"prompt_eval_count\": 146,\n",
      "              \"prompt_eval_duration\": 4000000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 47000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-21d91985-c776-487f-abf9-6622445e2de2-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [64ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3830599/622595501.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "question_router = Router_prompt | LOCAL_LLM | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Nodes(\n",
    "        retriever=retriever,\n",
    "        rag_chain=rag_chain,\n",
    "        retrieval_grader=retrieval_grader,\n",
    "        web_search_tool=web_search_tool,\n",
    "        question_router=question_router,\n",
    "        hallucination_grader=hallucination_grader,\n",
    "        answer_grader=answer_grader\n",
    "    )\n",
    "\n",
    "workflow = build_graph(nodes)\n",
    "app = workflow.compile()    # Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [1.04s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:40.482391816Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1037177243,\n",
      "          \"load_duration\": 938547545,\n",
      "          \"prompt_eval_count\": 150,\n",
      "          \"prompt_eval_duration\": 55000000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 40000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:40.482391816Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1037177243,\n",
      "              \"load_duration\": 938547545,\n",
      "              \"prompt_eval_count\": 150,\n",
      "              \"prompt_eval_duration\": 55000000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 40000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d3b414a7-58a2-4fa0-9a13-908db529c14a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [1.04s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "---ROUTE QUESTION TO RAG---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [1.04s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [1.04s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "---RETRIEVE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] [13ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: retrieve:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n            of a retrieved document to a user question. If the document contains keywords related to the user question, \\n            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n            \\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n            \\n            Here is the retrieved document: \\n            Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n            \\n            Here is the user question: \\n            What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [61ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:40.557257203Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 59608450,\n",
      "          \"load_duration\": 8835410,\n",
      "          \"prompt_eval_count\": 335,\n",
      "          \"prompt_eval_duration\": 8000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 42000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:40.557257203Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 59608450,\n",
      "              \"load_duration\": 8835410,\n",
      "              \"prompt_eval_count\": 335,\n",
      "              \"prompt_eval_duration\": 8000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 42000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d2b95795-ff59-46c2-a9eb-0e1e16d7c612-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [61ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n            of a retrieved document to a user question. If the document contains keywords related to the user question, \\n            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n            \\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n            \\n            Here is the retrieved document: \\n            Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n            \\n            Here is the user question: \\n            What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [45ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:40.603445731Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 44350889,\n",
      "          \"load_duration\": 10013149,\n",
      "          \"prompt_eval_count\": 335,\n",
      "          \"prompt_eval_duration\": 1000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 31000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:40.603445731Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 44350889,\n",
      "              \"load_duration\": 10013149,\n",
      "              \"prompt_eval_count\": 335,\n",
      "              \"prompt_eval_duration\": 1000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 31000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0be7219b-2f3c-49fd-8287-c42cfa57a82e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [46ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n            of a retrieved document to a user question. If the document contains keywords related to the user question, \\n            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n            \\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n            \\n            Here is the retrieved document: \\n            Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n            \\n            Here is the user question: \\n            What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [44ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:40.648586141Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 43360805,\n",
      "          \"load_duration\": 8924365,\n",
      "          \"prompt_eval_count\": 335,\n",
      "          \"prompt_eval_duration\": 1000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 32000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:40.648586141Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 43360805,\n",
      "              \"load_duration\": 8924365,\n",
      "              \"prompt_eval_count\": 335,\n",
      "              \"prompt_eval_duration\": 1000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 32000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0291a1cd-e3a5-4792-a31a-5393a454d088-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [45ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n            of a retrieved document to a user question. If the document contains keywords related to the user question, \\n            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n            \\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n            \\n            Here is the retrieved document: \\n            Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n            \\n            Here is the user question: \\n            What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [53ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:40.702252884Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 51800006,\n",
      "          \"load_duration\": 15740879,\n",
      "          \"prompt_eval_count\": 335,\n",
      "          \"prompt_eval_duration\": 1000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 33000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:40.702252884Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 51800006,\n",
      "              \"load_duration\": 15740879,\n",
      "              \"prompt_eval_count\": 335,\n",
      "              \"prompt_eval_duration\": 1000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 33000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b4afbb3c-4084-4617-a4e9-765a9033e631-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] [207ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "'Finished running: grade_documents:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"web_search\": \"Yes\",\n",
      "  \"documents\": []\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] Entering Tool run with input:\n",
      "\u001b[0m\"{'query': 'What are the types of agent memory?'}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] [2.92s] Exiting Tool run with output:\n",
      "\u001b[0m\"[{'url': 'https://www.restack.io/p/agent-architecture-answer-memory-management-ai-cat-ai', 'content': 'Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents'}, {'url': 'https://dev.to/akkiprime/towards-agi-part-1-agents-with-memory-4cp5', 'content': 'Deep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.'}, {'url': 'https://blog.langchain.dev/memory-for-agents/', 'content': 'Memory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?'}, {'url': 'https://www.autviz.com/types-of-ai-agents-reactive-limited-memory-and-advanced-agents/', 'content': 'AI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?'}, {'url': 'https://superagi.com/towards-agi-part-1/', 'content': 'Sign up for Latest SuperAGI Updates\\n\"*\" indicates required fields\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\ncommunity@superagi.com\\nFor Developers\\nDocs\\nGitHub\\nReleases\\nRoadmap\\nAPIs\\nCommunity\\nSupport Forum\\nMarketplace\\nSocial Mentions\\nReddit\\nCollectibles\\nResources\\nBlog\\nUse Cases\\nAGI Research Lab\\nTutorials\\nImportant Links\\nSuperAGI Cloud\\nApp Spotlight\\nSuperCoder\\nArchitecture\\n Check it out✨\\nFeatures\\nAction Console\\nResource Manager\\nTrajectory Fine-Tuning\\n\\u200c\\nMultiple Vector DBs\\nMulti-LLM Support\\nAgent Workflows\\nMarketplace\\nAgent Templates\\nDiscord\\nGitHub\\nTwitter\\nReddit\\nYoutube\\nTowards AGI (part 1): Agents with Memory\\nFebruary 6, 2024\\n7 mins read\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\nConclusion & Next Steps\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\nDeep dive into various types of Agent Memory\\nChoosing the right Memory design in Production\\nSince agents are powered by LLMs, they are inherently probabilistic.'}]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [2.92s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: websearch:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [418ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f6e4b2b2-3a67-41f0-8306-0cd785839826-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [420ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [96ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:44.146418625Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 94866311,\n",
      "          \"load_duration\": 9704579,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 32000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 51000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:44.146418625Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 94866311,\n",
      "              \"load_duration\": 9704579,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 32000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 51000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-bc6886e8-8743-4496-a7ec-c8f4b0e0d164-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [97ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [97ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [518ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [432ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-010d94c0-ef48-4eef-bd86-cf8aba1814f4-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [433ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [97ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:44.678123181Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 95863849,\n",
      "          \"load_duration\": 12030106,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 33000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 50000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:44.678123181Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 95863849,\n",
      "              \"load_duration\": 12030106,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 33000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 50000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-aa0c76a0-f448-4dd9-86d2-f8b1c15a7d92-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [98ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [98ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [531ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [410ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-22c552e6-2989-4ac6-8540-719c2ea1850a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [411ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [95ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:45.185116321Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 93643415,\n",
      "          \"load_duration\": 11798699,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 31000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 50000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:45.185116321Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 93643415,\n",
      "              \"load_duration\": 11798699,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 31000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 50000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-22f58d67-5625-4d38-80cd-7eff0d7bf808-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [96ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [96ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [507ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [437ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4949ca90-2b49-4a5c-a5db-26690a83ca50-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [438ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [102ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:45.726723633Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 100395591,\n",
      "          \"load_duration\": 10188688,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 40000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 49000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:45.726723633Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 100395591,\n",
      "              \"load_duration\": 10188688,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 40000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 49000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-aaa6146e-a20a-4b5b-a5c9-c31b789f3404-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [103ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [103ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [541ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [427ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-484556c0-d926-4c79-a23b-77e88579fd23-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [428ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [102ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:46.258412565Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 100920602,\n",
      "          \"load_duration\": 9002403,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 41000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 50000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:46.258412565Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 100920602,\n",
      "              \"load_duration\": 9002403,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 41000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 50000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3de3b00c-18c5-4ce9-8b31-ea6786b42713-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [103ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [103ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [531ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [401ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b799a29c-ab0d-45ce-b831-7aa31c0b93b1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [402ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [95ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:46.757501487Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 93935731,\n",
      "          \"load_duration\": 10871616,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 32000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 49000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:46.757501487Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 93935731,\n",
      "              \"load_duration\": 10871616,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 32000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 49000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8196ef12-e7e2-4338-916f-3da54d71da8b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [96ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [97ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [499ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [420ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-711b554e-7bf8-42ad-9c5b-711cbdd2ed22-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [421ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [96ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:47.27618519Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 94896864,\n",
      "          \"load_duration\": 9091957,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 35000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 49000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:47.27618519Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 94896864,\n",
      "              \"load_duration\": 9091957,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 35000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 49000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a37d7455-063b-4dff-ae38-ca158c73ae64-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [97ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [97ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [518ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [441ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9bdf069f-a63d-495e-b46e-73daf9b3d074-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [442ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [131ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:47.851057647Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 130323933,\n",
      "          \"load_duration\": 8907661,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 88000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 32000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:47.851057647Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 130323933,\n",
      "              \"load_duration\": 8907661,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 88000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 32000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-fdf8d57d-0fdf-4c30-bc60-46a30352c1c2-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [132ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [132ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [575ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [374ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0cd49279-26a5-4745-9f05-f4af94720b2a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [375ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:48.276812583Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 48022122,\n",
      "          \"load_duration\": 9105093,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:48.276812583Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 48022122,\n",
      "              \"load_duration\": 9105093,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-089b6e81-ec22-4ece-b5f8-206e3c65a10a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [426ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [364ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-368f96d9-9b89-44f3-b75e-53f54c158597-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [365ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [53ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:48.696746637Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 51982290,\n",
      "          \"load_duration\": 9079279,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 7000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:48.696746637Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 51982290,\n",
      "              \"load_duration\": 9079279,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 7000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a903d6e4-1024-4584-a1ad-300329c6799f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [420ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [377ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-62e16025-eb6f-4b2e-b81c-bae7ac6f25b7-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [378ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:49.126293582Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 48197630,\n",
      "          \"load_duration\": 9028546,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:49.126293582Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 48197630,\n",
      "              \"load_duration\": 9028546,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-031ea282-0c57-4c15-907c-2963d98141c5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [51ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [429ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [358ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-78464d1d-26c1-431b-b233-9398cd2d5f06-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [359ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [55ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:49.54226324Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 53751772,\n",
      "          \"load_duration\": 15881426,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:49.54226324Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 53751772,\n",
      "              \"load_duration\": 15881426,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5cda22f5-d651-4897-bd04-2a26d9179f92-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [56ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [56ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [416ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [362ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-31b5c034-28ef-499f-9c6a-36600733ec8c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [363ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:49.955584391Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47767579,\n",
      "          \"load_duration\": 9414351,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:49.955584391Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47767579,\n",
      "              \"load_duration\": 9414351,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-85e055fe-8b23-4a6c-8fab-f418dfea942c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [413ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [352ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-2a6bb1ed-e9ee-42b1-b479-bd8092285d0a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [353ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [53ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:50.363369768Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 52028298,\n",
      "          \"load_duration\": 14251846,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:50.363369768Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 52028298,\n",
      "              \"load_duration\": 14251846,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a734f65b-0164-4ce7-bbb8-09c32e67264a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [408ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [386ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5c883be9-5b2b-46fb-bfa7-53f5a421fd70-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [387ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [51ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:50.803086729Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 49795454,\n",
      "          \"load_duration\": 11497300,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:50.803086729Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 49795454,\n",
      "              \"load_duration\": 11497300,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-befc9c3f-ad5e-454d-82db-9c994cf145b3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [52ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [52ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [439ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [372ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5c5b1248-ffbb-4214-9bc5-d74c6cb523ae-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [373ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [54ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:51.231956917Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 52216405,\n",
      "          \"load_duration\": 8993486,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 7000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:51.231956917Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 52216405,\n",
      "              \"load_duration\": 8993486,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 7000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4ed586f9-7834-4e41-a8f3-9bb086dac709-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [429ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [361ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c4ee3f9a-ec57-49e0-a273-c93abb8b4721-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [361ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:51.6442069Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 48230378,\n",
      "          \"load_duration\": 8912554,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:51.6442069Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 48230378,\n",
      "              \"load_duration\": 8912554,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7ec6560a-d41f-467b-aa77-26da8983c7aa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [412ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [375ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-60f4c0bd-03db-4c04-97be-3782da1662b8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [376ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [54ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:52.07507867Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 52455727,\n",
      "          \"load_duration\": 9129143,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 7000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:52.07507867Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 52455727,\n",
      "              \"load_duration\": 9129143,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 7000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-913f9121-bd9c-4b09-af05-c53176df748f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [431ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [358ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-93d3e7ea-8432-41d9-91bf-603be2f61441-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [359ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [62ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:52.497722992Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 60206007,\n",
      "          \"load_duration\": 15027864,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 10000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:52.497722992Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 60206007,\n",
      "              \"load_duration\": 15027864,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 10000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-32aa6a77-bb14-47c2-a141-f1918861ba53-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [63ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [63ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [422ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [370ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c4d8f3ea-9c2a-4557-8c5c-6d1808c1a7f5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [371ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [54ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:52.924383782Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 52723588,\n",
      "          \"load_duration\": 8861351,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 8000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:52.924383782Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 52723588,\n",
      "              \"load_duration\": 8861351,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 8000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5def09e5-26a4-42a8-b44c-61945dc79219-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [426ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [380ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b9deaa67-a3e5-4cfb-8e4f-404ac9334509-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [381ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [54ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:53.360987422Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 52713271,\n",
      "          \"load_duration\": 9082016,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 8000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:53.360987422Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 52713271,\n",
      "              \"load_duration\": 9082016,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 8000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a1a4dcd5-9180-45be-aa1d-08c2e103d841-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [55ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [436ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [362ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9068ffe5-45d5-4485-be84-6794577a4ada-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [363ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [69ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:53.795399058Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 67535279,\n",
      "          \"load_duration\": 8994529,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 16000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 41000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:53.795399058Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 67535279,\n",
      "              \"load_duration\": 8994529,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 16000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 41000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-88862b35-9216-4c7c-9b32-5a9fed886fea-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [70ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [70ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [434ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [356ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-57443b25-1422-4568-8df0-997a838c36fa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [357ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:54.202315561Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47152089,\n",
      "          \"load_duration\": 9087608,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:54.202315561Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47152089,\n",
      "              \"load_duration\": 9087608,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ce24bd2a-91bc-4940-a9c1-e04eb3479730-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [407ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [354ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-718fece3-c3cc-4e85-b0ca-c4827ddba30f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [355ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:54.608026688Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47797948,\n",
      "          \"load_duration\": 8978132,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:54.608026688Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47797948,\n",
      "              \"load_duration\": 8978132,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-defd6527-00d8-44f7-9857-52b580210927-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [406ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [382ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-71db3530-8e86-49df-803c-4ce655d6277e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [383ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:55.04153376Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47781573,\n",
      "          \"load_duration\": 8975788,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:55.04153376Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47781573,\n",
      "              \"load_duration\": 8975788,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c8619667-e1a0-476b-8daf-ad8a62881fef-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [434ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [373ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5a65a274-f655-40d2-ab98-ac5e4d8b9f3d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [374ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:55.465818815Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47215202,\n",
      "          \"load_duration\": 9245511,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:55.465818815Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47215202,\n",
      "              \"load_duration\": 9245511,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-950dd839-0fe2-4dde-a7c4-8dc9a69c4df5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [424ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [369ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-57dd6a31-d071-438b-bfd8-5cd238c95b4a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [370ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [55ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:55.893162151Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 53695072,\n",
      "          \"load_duration\": 15875351,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:55.893162151Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 53695072,\n",
      "              \"load_duration\": 15875351,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ffd934dc-6245-455e-a319-491d705f3127-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [57ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [57ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [427ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [379ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-23ff1c7b-33e4-4f66-8b5c-0be951ae4709-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [380ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:56.323722924Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47899666,\n",
      "          \"load_duration\": 8890679,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:56.323722924Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47899666,\n",
      "              \"load_duration\": 8890679,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-64419d6f-8c1b-4d4e-8415-132e28ccd03f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [430ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [379ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-621698cc-2e26-4992-86bc-2e6498f86151-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [380ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [59ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:56.7644846Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 58022284,\n",
      "          \"load_duration\": 9138432,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 16000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 31000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:56.7644846Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 58022284,\n",
      "              \"load_duration\": 9138432,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 16000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 31000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-149959be-7b0b-4131-8b97-7d0f7bb3f410-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [440ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [376ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0579ad9e-a7a7-4947-bdd8-60ce3fae20be-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [377ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:57.192586406Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47715119,\n",
      "          \"load_duration\": 8854070,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:57.192586406Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47715119,\n",
      "              \"load_duration\": 8854070,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4f81a877-2755-4468-accd-980343f6dd10-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [427ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [372ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6751f235-2ddd-465d-9e4c-2201eda271b5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [373ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:57.616186057Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 46823656,\n",
      "          \"load_duration\": 8830423,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:57.616186057Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 46823656,\n",
      "              \"load_duration\": 8830423,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b5fa5a0b-0bdc-4f6b-a0a8-2e9eca35c826-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [423ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [374ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-55cd33b0-812a-433f-abfd-07030629965c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [374ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [70ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:58.062743615Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 68775099,\n",
      "          \"load_duration\": 10686277,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 15000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 42000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:58.062743615Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 68775099,\n",
      "              \"load_duration\": 10686277,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 15000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 42000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7c53d7ff-4f62-41fd-afa1-1ee3bf2ed437-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [71ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [72ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [447ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [353ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6dea5f26-97b2-4a41-8a78-6c889c686dc5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [354ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [58ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:58.477535988Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 56761099,\n",
      "          \"load_duration\": 14206610,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 6000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:58.477535988Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 56761099,\n",
      "              \"load_duration\": 14206610,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 6000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-448a5b27-cc14-4746-b362-995eb2ac968f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [415ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [362ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8ff30356-c99f-4380-b39f-8b574e37271d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [363ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:58.890604596Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 46921179,\n",
      "          \"load_duration\": 9170203,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:58.890604596Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 46921179,\n",
      "              \"load_duration\": 9170203,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-39671049-2e78-4b52-b620-16286e302007-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [412ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [364ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-691e4cfe-68fb-4119-bb80-885175d840aa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [364ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [52ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:59.309088684Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 51412902,\n",
      "          \"load_duration\": 8982948,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 7000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:59.309088684Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 51412902,\n",
      "              \"load_duration\": 8982948,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 7000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-551a4796-994a-48f7-845d-c21fbdc4a1bd-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [53ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [53ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [418ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [377ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a98f5f08-09e6-4ac9-a29c-512c5e4ad616-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [378ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [71ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:25:59.759410705Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 69862827,\n",
      "          \"load_duration\": 12912170,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 13000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 42000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:25:59.759410705Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 69862827,\n",
      "              \"load_duration\": 12912170,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 13000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 42000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3e4aa489-216e-4842-851d-d7be6b758bc9-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [72ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [72ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [450ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [373ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b4f04c7b-fc4e-437e-a647-b429df62d6c3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [373ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:00.183687936Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 48029952,\n",
      "          \"load_duration\": 9950265,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:00.183687936Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 48029952,\n",
      "              \"load_duration\": 9950265,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e93eeca0-905d-4fd7-9875-5570e6cbc01b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [424ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [354ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-98316b5f-e935-4ba3-8d7d-706e43cfba4e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [355ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [53ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:00.593824957Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 51986649,\n",
      "          \"load_duration\": 13891039,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:00.593824957Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 51986649,\n",
      "              \"load_duration\": 13891039,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-89201e5a-836b-4220-adaf-ee661df9a1e9-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [410ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [358ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-61f33874-694a-47a9-a362-86db6af5b709-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [359ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:01.002988771Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47809347,\n",
      "          \"load_duration\": 8921129,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:01.002988771Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47809347,\n",
      "              \"load_duration\": 8921129,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-62883ede-a4f3-4da6-a2ff-e0a4b87b2192-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [409ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [370ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-92496f65-4b15-4ee4-a0ad-e4f163e0aa0c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [370ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:01.42308036Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 46967180,\n",
      "          \"load_duration\": 9000631,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:01.42308036Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 46967180,\n",
      "              \"load_duration\": 9000631,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ebc5104e-bb90-4d06-9ab8-bfaf62cd1c56-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [420ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [363ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-bbefc21e-5f39-4ca2-983c-76f231bba3e9-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [364ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [52ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:01.841254332Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 50944067,\n",
      "          \"load_duration\": 9770134,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 5000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:01.841254332Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 50944067,\n",
      "              \"load_duration\": 9770134,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 5000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c99d9de3-db32-4030-a020-ff8d0ee59b2e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [53ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [53ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [418ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [373ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-57092446-7801-46c1-9b0a-2de14eadaf1b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [374ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [59ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:02.275725793Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 57665449,\n",
      "          \"load_duration\": 9009958,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 13000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:02.275725793Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 57665449,\n",
      "              \"load_duration\": 9009958,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 13000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9743875a-9c86-4cf3-9d51-a722abd83105-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [434ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [366ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-84758b5f-eac4-4493-858c-78ccb3ea70f7-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [366ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [53ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:02.696338971Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 51432279,\n",
      "          \"load_duration\": 8987253,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 6000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 34000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:02.696338971Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 51432279,\n",
      "              \"load_duration\": 8987253,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 6000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 34000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-22a3dda3-0118-4acb-8310-a9522bee2806-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [54ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [421ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [362ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-58b98ea1-4c2f-4862-bda2-9a858e451af8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [363ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [48ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:03.108986915Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 46877500,\n",
      "          \"load_duration\": 9026132,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 2000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:03.108986915Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 46877500,\n",
      "              \"load_duration\": 9026132,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 2000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1dde5bf0-312d-4307-8ce5-01f9cd0002e2-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [49ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [412ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [376ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-50194583-48da-43ba-a398-0455d7391b4f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [377ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [58ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:03.546183869Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 56727305,\n",
      "          \"load_duration\": 8936916,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 12000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:03.546183869Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 56727305,\n",
      "              \"load_duration\": 8936916,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 12000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0cd062ef-74cb-41bc-aef1-ea88beffca20-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [59ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [60ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [437ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [365ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d7f7b10b-d1b0-485e-b91f-7a4e3161574e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [366ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:03.963285957Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 48218039,\n",
      "          \"load_duration\": 9304097,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:03.963285957Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 48218039,\n",
      "              \"load_duration\": 9304097,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0664b155-c811-4923-aead-789fce7b0bfc-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [51ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [417ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [372ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-dcffb1eb-c592-4f30-b1da-7a29ad08c4f0-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [373ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{}   \\n\\n   \\n   \\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n   \\n\\n\\n  \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={}, page_content='Memory Management In AI Agents Memory Management In AI Agents Explore memory management techniques in AI agents, focusing on efficiency and optimization for enhanced performance. Types of Memory in AI Agents Types of Memory in AI Agents ---------------------------- Memory management in AI agents incorporates short-term memory for context and long-term memory using external storage, such as vector databases, enabling efficient information retrieval and enhanced reasoning. Memory management in AI agents is crucial for optimizing performance and enhancing user interactions. By leveraging short-term, long-term, and contextual memory, agents can provide more personalized and efficient interactions, ultimately leading to better outcomes for users. This ongoing development aims to enhance memory management in AI agents, ensuring they can provide more coherent and contextually relevant interactions. Types of Memory in AI Agents\\\\nDeep dive into various types of Agent Memory STM: Working memory (LLM Context): It is a data structure with multiple parts which are usually represented with a prompt template and relevant variables.Before runtime, the STM is synthesized by replacing the relevant variables in the prompt template with information retrieved from the LTM.\\\\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?\\\\nAI Agents  Types of AI Agents: Reactive, Limited Memory, and Advanced Agents  Imagine boosting labor productivity by 1% every year simply by integrating intelligent agents into your operations. That’s not all—Generative AI agents are projected to automate 15% to 50% of business functions by 2027, completely redefining efficiency across industries. Be it automating mundane tasks or making complex decisions, AI agents have become an integral part of modern operations. What Are AI Agents? Limited Memory agents offer much value when personalization and adaptability are high. Advanced Agents: The Future of AI  Comparison of AI Agents Understanding types of AI agents such as Reactive, Limited Memory, and Advanced will help corporations use their potential appropriately. Interested in how AI agents can revolutionize your business?\\\\nSign up for Latest SuperAGI Updates\\\\n\\\"*\\\" indicates required fields\\\\nSuperAGI builds infrastructure components, tools, frameworks and models to enable opensource AGI\\\\ncommunity@superagi.com\\\\nFor Developers\\\\nDocs\\\\nGitHub\\\\nReleases\\\\nRoadmap\\\\nAPIs\\\\nCommunity\\\\nSupport Forum\\\\nMarketplace\\\\nSocial Mentions\\\\nReddit\\\\nCollectibles\\\\nResources\\\\nBlog\\\\nUse Cases\\\\nAGI Research Lab\\\\nTutorials\\\\nImportant Links\\\\nSuperAGI Cloud\\\\nApp Spotlight\\\\nSuperCoder\\\\nArchitecture\\\\n Check it out✨\\\\nFeatures\\\\nAction Console\\\\nResource Manager\\\\nTrajectory Fine-Tuning\\\\n\\\\u200c\\\\nMultiple Vector DBs\\\\nMulti-LLM Support\\\\nAgent Workflows\\\\nMarketplace\\\\nAgent Templates\\\\nDiscord\\\\nGitHub\\\\nTwitter\\\\nReddit\\\\nYoutube\\\\nTowards AGI (part 1): Agents with Memory\\\\nFebruary 6, 2024\\\\n7 mins read\\\\nAgents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. In a professional setup like this, the agent is responsible for extracting tasks from conversations and passing them to an employee and once the employee completes the task the agent will convey the output to the user. Then use the solutions of those basic tasks as in-context examples to solve the current task.\\\\nConclusion & Next Steps\\\\nIn this blog, we saw that design choices for Memory depend on the end use case. This analogy is better captured in the following table:\\\\nDeep dive into various types of Agent Memory\\\\nChoosing the right Memory design in Production\\\\nSince agents are powered by LLMs, they are inherently probabilistic.')] \\n\\n    Here is the answer: \\n    {}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [49ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b\",\n",
      "          \"created_at\": \"2024-12-20T06:26:04.387276793Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 47974912,\n",
      "          \"load_duration\": 9083783,\n",
      "          \"prompt_eval_count\": 971,\n",
      "          \"prompt_eval_duration\": 3000000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 35000000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b\",\n",
      "              \"created_at\": \"2024-12-20T06:26:04.387276793Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 47974912,\n",
      "              \"load_duration\": 9083783,\n",
      "              \"prompt_eval_count\": 971,\n",
      "              \"prompt_eval_duration\": 3000000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 35000000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4369f269-1d7d-4fab-9593-90f26274b917-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [50ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [424ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:LangGraph] [24.95s] Chain run errored with error:\n",
      "\u001b[0m\"GraphRecursionError('Recursion limit of 50 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\\\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/ollama_platform/miniconda3/envs/AgenticRAG/lib/python3.12/site-packages/langgraph/pregel/__init__.py\\\", line 1669, in stream\\n    raise GraphRecursionError(msg)\\n\\n\\nlanggraph.errors.GraphRecursionError: Recursion limit of 50 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\"\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 50 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the types of agent memory?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinished running: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AgenticRAG/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1669\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1661\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   1662\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1663\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   1668\u001b[0m     )\n\u001b[0;32m-> 1669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 50 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "config = RunnableConfig(recursion_limit=50)\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs, config):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
